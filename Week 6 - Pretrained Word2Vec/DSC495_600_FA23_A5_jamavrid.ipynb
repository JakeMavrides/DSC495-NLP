{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0uD2e04bhIS"
      },
      "source": [
        "[Pretrained Word2Vec Embeddings](#Pretrained-Word2Vec-Embeddings)   \n",
        "\n",
        "1. [Google News Embedding](#Google-News-Embedding)    \n",
        "2. [Similarity Scores](#Similarity-Scores)    \n",
        "3. [Debunking Some Common Examples](#Debunking-Some-Common-Examples)\n",
        "4. [Other Pretrained Models](Other-Pretrained-Models)\n",
        "5. [Building Your Own Model](Building-Your-Own-Model)\n",
        "\n",
        "\n",
        "\n",
        "# Pretrained Word2Vec Embeddings\n",
        "\n",
        "Now that we have a better feel for the model behind Word2Vec we'll see how we can implement word embeddings developed by teams with more computational resources than most everyone has at home.\n",
        "\n",
        "We'll be using `gensim` and more or less working through the tutorial in their documentation here https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html. I'll also try my best to pull in some examples from Mikolov et. al.'s original papers.\n",
        "\n",
        "First things first, please try running the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "itxvNAJYbhIT"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LV1PS_28bhIU"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWDq5FfGbhIU",
        "outputId": "5a153b83-79f6-4414-9672-79bf52395aa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "wv = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp9istxabhIU"
      },
      "source": [
        "The above code takes a bit to run, and takes a long while to run if it's your first time running it (the model is approximately 2GB). So please hit shift + enter now while I'm talking so it will load in time for our first coding break.\n",
        "\n",
        "## Google News Embedding\n",
        "The model we just loaded using gensim was built by Google using their Google News dataset. Their network was trained with a vocabulary of 3 million words and phrases using.\n",
        "\n",
        "These 3 million dimensional one hot encoded vectors were then projected down to 300 features, meaning that each word vector in the embedding has 300 dimensions. Using the language from `Last week's class` their $M = 3,000,000$ and their #N= 300#.\n",
        "\n",
        "### Accessing the Word Vectors\n",
        "Once the above has loaded you can access the embedding of any word or phrase in the embedding by simply calling wv[string]. Let's see"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCfFiFybbhIU",
        "outputId": "ea1ad591-db26-4681-dce8-461d5bf668d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.32617188,  0.13085938,  0.03466797, -0.08300781,  0.08984375,\n",
              "       -0.04125977, -0.19824219,  0.00689697,  0.14355469,  0.0019455 ,\n",
              "        0.02880859, -0.25      , -0.08398438, -0.15136719, -0.10205078,\n",
              "        0.04077148, -0.09765625,  0.05932617,  0.02978516, -0.10058594,\n",
              "       -0.13085938,  0.001297  ,  0.02612305, -0.27148438,  0.06396484,\n",
              "       -0.19140625, -0.078125  ,  0.25976562,  0.375     , -0.04541016,\n",
              "        0.16210938,  0.13671875, -0.06396484, -0.02062988, -0.09667969,\n",
              "        0.25390625,  0.24804688, -0.12695312,  0.07177734,  0.3203125 ,\n",
              "        0.03149414, -0.03857422,  0.21191406, -0.00811768,  0.22265625,\n",
              "       -0.13476562, -0.07617188,  0.01049805, -0.05175781,  0.03808594,\n",
              "       -0.13378906,  0.125     ,  0.0559082 , -0.18261719,  0.08154297,\n",
              "       -0.08447266, -0.07763672, -0.04345703,  0.08105469, -0.01092529,\n",
              "        0.17480469,  0.30664062, -0.04321289, -0.01416016,  0.09082031,\n",
              "       -0.00927734, -0.03442383, -0.11523438,  0.12451172, -0.0246582 ,\n",
              "        0.08544922,  0.14355469, -0.27734375,  0.03662109, -0.11035156,\n",
              "        0.13085938, -0.01721191, -0.08056641, -0.00708008, -0.02954102,\n",
              "        0.30078125, -0.09033203,  0.03149414, -0.18652344, -0.11181641,\n",
              "        0.10253906, -0.25976562, -0.02209473,  0.16796875, -0.05322266,\n",
              "       -0.14550781, -0.01049805, -0.03039551, -0.03857422,  0.11523438,\n",
              "       -0.0062561 , -0.13964844,  0.08007812,  0.06103516, -0.15332031,\n",
              "       -0.11132812, -0.14160156,  0.19824219, -0.06933594,  0.29296875,\n",
              "       -0.16015625,  0.20898438,  0.00041771,  0.01831055, -0.20214844,\n",
              "        0.04760742,  0.05810547, -0.0123291 , -0.01989746, -0.00364685,\n",
              "       -0.0135498 , -0.08251953, -0.03149414,  0.00717163,  0.20117188,\n",
              "        0.08300781, -0.0480957 , -0.26367188, -0.09667969, -0.22558594,\n",
              "       -0.09667969,  0.06494141, -0.02502441,  0.08496094,  0.03198242,\n",
              "       -0.07568359, -0.25390625, -0.11669922, -0.01446533, -0.16015625,\n",
              "       -0.00701904, -0.05712891,  0.02807617, -0.09179688,  0.25195312,\n",
              "        0.24121094,  0.06640625,  0.12988281,  0.17089844, -0.13671875,\n",
              "        0.1875    , -0.10009766, -0.04199219, -0.12011719,  0.00524902,\n",
              "        0.15625   , -0.203125  , -0.07128906, -0.06103516,  0.01635742,\n",
              "        0.18261719,  0.03588867, -0.04248047,  0.16796875, -0.15039062,\n",
              "       -0.16992188,  0.01831055,  0.27734375, -0.01269531, -0.0390625 ,\n",
              "       -0.15429688,  0.18457031, -0.07910156,  0.09033203, -0.02709961,\n",
              "        0.08251953,  0.06738281, -0.16113281, -0.19628906, -0.15234375,\n",
              "       -0.04711914,  0.04760742,  0.05908203, -0.16894531, -0.14941406,\n",
              "        0.12988281,  0.04321289,  0.02624512, -0.1796875 , -0.19628906,\n",
              "        0.06445312,  0.08935547,  0.1640625 , -0.03808594, -0.09814453,\n",
              "       -0.01483154,  0.1875    ,  0.12792969,  0.22753906,  0.01818848,\n",
              "       -0.07958984, -0.11376953, -0.06933594, -0.15527344, -0.08105469,\n",
              "       -0.09277344, -0.11328125, -0.15136719, -0.08007812, -0.05126953,\n",
              "       -0.15332031,  0.11669922,  0.06835938,  0.0324707 , -0.33984375,\n",
              "       -0.08154297, -0.08349609,  0.04003906,  0.04907227, -0.24121094,\n",
              "       -0.13476562, -0.05932617,  0.12158203, -0.34179688,  0.16503906,\n",
              "        0.06176758, -0.18164062,  0.20117188, -0.07714844,  0.1640625 ,\n",
              "        0.00402832,  0.30273438, -0.10009766, -0.13671875, -0.05957031,\n",
              "        0.0625    , -0.21289062, -0.06542969,  0.1796875 , -0.07763672,\n",
              "       -0.01928711, -0.15039062, -0.00106049,  0.03417969,  0.03344727,\n",
              "        0.19335938,  0.01965332, -0.19921875, -0.10644531,  0.01525879,\n",
              "        0.00927734,  0.01416016, -0.02392578,  0.05883789,  0.02368164,\n",
              "        0.125     ,  0.04760742, -0.05566406,  0.11572266,  0.14746094,\n",
              "        0.1015625 , -0.07128906, -0.07714844, -0.12597656,  0.0291748 ,\n",
              "        0.09521484, -0.12402344, -0.109375  , -0.12890625,  0.16308594,\n",
              "        0.28320312, -0.03149414,  0.12304688, -0.23242188, -0.09375   ,\n",
              "       -0.12988281,  0.0135498 , -0.03881836, -0.08251953,  0.00897217,\n",
              "        0.16308594,  0.10546875, -0.13867188, -0.16503906, -0.03857422,\n",
              "        0.10839844, -0.10498047,  0.06396484,  0.38867188, -0.05981445,\n",
              "       -0.0612793 , -0.10449219, -0.16796875,  0.07177734,  0.13964844,\n",
              "        0.15527344, -0.03125   , -0.20214844, -0.12988281, -0.10058594,\n",
              "       -0.06396484, -0.08349609, -0.30273438, -0.08007812,  0.02099609],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "wv['man']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRDMKcdTbhIV"
      },
      "outputs": [],
      "source": [
        "wv['woman']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMhJA-6_bhIV"
      },
      "outputs": [],
      "source": [
        "wv['influenza']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU7ME2UDbhIV"
      },
      "source": [
        "### Retrieving the index\n",
        "It would be nice to know whether or not a word/phrase we're interested in is in the vocabulary. We can check by looking at the word index for the vocabulary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dbfd2hKIbhIV",
        "outputId": "ccc8041a-8bb7-4af5-96ab-e062d759b1e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s>\n",
            "in\n",
            "for\n",
            "that\n",
            "is\n",
            "on\n",
            "##\n",
            "The\n",
            "with\n",
            "said\n",
            "was\n",
            "the\n",
            "at\n",
            "not\n",
            "as\n",
            "it\n",
            "be\n",
            "from\n",
            "by\n",
            "are\n"
          ]
        }
      ],
      "source": [
        "# index2word contains every word in the\n",
        "# vocabulary\n",
        "\n",
        "# let's look at the first 20\n",
        "for i in range(20):\n",
        "    print(wv.index_to_key[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61SHV7TcbhIV",
        "outputId": "70b4b46f-23a0-4db6-d1a2-608e9af5b557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said', 'was', 'the', 'at', 'not', 'as', 'it', 'be', 'from', 'by', 'are']\n"
          ]
        }
      ],
      "source": [
        "# wv.index2word is a list of the words/phrases in the vocab\n",
        "print(wv.index_to_key[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_Iw-6mXbhIV"
      },
      "source": [
        "## Similarity Scores\n",
        "Similar to LSA Word2Vec can be used to find words that are similar to one another, which is useful for tasks like searching through a document for words related to a particular topic.\n",
        "\n",
        "Work through the following to learn how to use the pretrained Word2Vec to find both the similarity between pairs of words, as well as find the most similar words to a predefined set.\n",
        "\n",
        "### Calculating Similarities between pairs of word embeddings\n",
        "There are a few different ways you can calculate similarity scores between pairs of vectors. You'll work through them now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVZ0Af5dbhIV",
        "outputId": "02959dc5-16b1-44e9-d167-4c2f62c04d09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6058261"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "### similarity(word1, word2)\n",
        "## call wv.similarity for two words/phrases\n",
        "## Try and find the similarity between \"apple\" and\n",
        "## other fruits and vegetables you know\n",
        "wv.similarity('apple', 'strawberry')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtjmBXzjbhIW",
        "outputId": "06799229-ccda-4d72-9538-c90beee5823c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.56081817, 0.60582612])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "### cosine_similairties(vec1, array_of_vectors)\n",
        "## call wv.cosine_similarities for a vector and a collection of vectors\n",
        "## Create a numpy array where each row has the word embeddings\n",
        "## for the fruits and vegetables you were interested in\n",
        "## compare to the word embedding for \"apple\"\n",
        "embedding = np.zeros([2, 300])\n",
        "embedding[0,:] = wv['tomato']\n",
        "embedding[1,:] = wv['strawberry']\n",
        "wv.cosine_similarities(wv['apple'], embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpdWmSbfbhIW"
      },
      "source": [
        "### Finding the most similar words\n",
        "Another problem you may be interested in is finding the words that are most similar to a given word or vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qdku66oZbhIW",
        "outputId": "377efc38-2597-4a62-9949-124c96dbe6f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('apples', 0.720359742641449),\n",
              " ('pear', 0.6450697183609009),\n",
              " ('fruit', 0.6410146355628967),\n",
              " ('berry', 0.6302295327186584),\n",
              " ('pears', 0.613396167755127),\n",
              " ('strawberry', 0.6058260798454285),\n",
              " ('peach', 0.6025872826576233),\n",
              " ('potato', 0.5960935354232788),\n",
              " ('grape', 0.5935863852500916),\n",
              " ('blueberry', 0.5866668224334717)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "## You can find the most similar words to a given word\n",
        "## using wv.similar_by_word(word)\n",
        "## try it out with \"apple\"\n",
        "wv.similar_by_word('apple')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8F4fqt8bhIW",
        "outputId": "bd8ed7ca-f580-46f2-c857-39042efd53c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('apples', 0.720359742641449),\n",
              " ('pear', 0.6450697183609009),\n",
              " ('fruit', 0.6410146355628967),\n",
              " ('berry', 0.6302295327186584),\n",
              " ('pears', 0.613396167755127),\n",
              " ('strawberry', 0.6058260798454285),\n",
              " ('peach', 0.6025872826576233),\n",
              " ('potato', 0.5960935354232788),\n",
              " ('grape', 0.5935863852500916),\n",
              " ('blueberry', 0.5866668224334717),\n",
              " ('cherries', 0.5784382820129395),\n",
              " ('mango', 0.5751855969429016),\n",
              " ('apricot', 0.5727777481079102),\n",
              " ('melon', 0.5719985365867615),\n",
              " ('almond', 0.5704829692840576),\n",
              " ('Granny_Smiths', 0.5695334076881409),\n",
              " ('grapes', 0.5692256093025208),\n",
              " ('peaches', 0.5659247040748596),\n",
              " ('pumpkin', 0.5651883482933044),\n",
              " ('apricots', 0.5645568370819092),\n",
              " ('berries', 0.5636081099510193),\n",
              " ('tomato', 0.5608181953430176),\n",
              " ('honeycrisp', 0.5566703677177429),\n",
              " ('mango_tango', 0.5529055595397949),\n",
              " ('apple_pear', 0.5523689985275269)]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "## You can control how many words are returned with topn\n",
        "## Get the 25 most similar words to \"apple\"\n",
        "wv.most_similar('apple', topn=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYH6JroNbhIW",
        "outputId": "22a04d43-b0ce-411d-8507-c1b571cf4213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('apples', 0.6275594234466553), ('blackberry_pie', 0.6264823079109192), ('pumpkin', 0.6011644005775452), ('pear', 0.600442111492157), ('eclair', 0.5951197743415833), ('cherry_pie', 0.5863113403320312), ('pies', 0.5823993682861328), ('potato', 0.5805648565292358), ('watermelon', 0.5803582668304443), ('pumpkin_pecan', 0.5800970792770386)]\n",
            "[('computers', 0.5846256017684937), ('laptop', 0.5782973170280457), ('Apple_IIe', 0.5261160731315613), ('apples', 0.5179873108863831), ('iMac', 0.513472855091095), ('receive_MacMall_Exclusive', 0.5100300908088684), ('laptop_computer', 0.5042001008987427), ('cartoonish_apple', 0.4981754422187805), ('iBook_laptop', 0.49246513843536377), ('Macbook_laptop', 0.4881983995437622)]\n"
          ]
        }
      ],
      "source": [
        "## You can also do this for a collection of words\n",
        "## with wv.most_similar(list_of_words)\n",
        "## try running this on the list [\"apple\",\"pie\"]\n",
        "## and the list [\"apple\",\"computer\"]\n",
        "print(wv.most_similar(['apple','pie']))\n",
        "print(wv.most_similar(['apple','computer']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdBvQfvzbhIW",
        "outputId": "596112e9-d671-416a-e67b-16753a7a2d44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hyatt_Van_Cotthem', 0.23481279611587524),\n",
              " ('Pat_Kreitlow', 0.22138920426368713),\n",
              " ('By_CYNTHIA_BULLION', 0.22118937969207764),\n",
              " ('Shediac_Marina', 0.218452587723732),\n",
              " ('Rep._Jeff_Mursau', 0.21573235094547272),\n",
              " ('Rep._Michelle_Litjens', 0.21493346989154816),\n",
              " ('developer_Jerry_Trooien', 0.2126341015100479),\n",
              " ('Lars_Christiansen', 0.21195879578590393),\n",
              " ('Joe_Leibham', 0.20946094393730164),\n",
              " ('Changing_Lanes_looks', 0.20705245435237885)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "## Sometimes you may not have a word, but rather a vector\n",
        "## You can find the words most similar to that vector with\n",
        "## wv.similar_by_vector(vec)\n",
        "## Find the words most similar to the test vector below\n",
        "test = 2*np.random.random(300)-1\n",
        "\n",
        "wv.similar_by_vector(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ988CeJbhIW"
      },
      "source": [
        "One of these things just doesn't belong\n",
        "Another fun feature is that you can put in a list of words and Word2Vec can pick out the one that doesn't belong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyBUepHqbhIW",
        "outputId": "c7889bd2-17cb-4bbf-d296-374060c203b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jeans\n"
          ]
        }
      ],
      "source": [
        "print(wv.doesnt_match([\"apple\",\"banana\",\"grapes\",\"pear\",\"jeans\"]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv.similarity('apple', 'jeans') # .122 not very similar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALSLIz9Gh5Gc",
        "outputId": "a5801c3f-e92e-48f5-86dc-1845267009ce"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12228806"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTKdm-p1bhIW"
      },
      "source": [
        "## Debunking Some Common Examples\n",
        "If you've ever heard of Word2Vec prior to this, you may have seen amazingly intuitive results like \"King\" - \"Man\" + \"Woman\" = \"Queen\".\n",
        "\n",
        "While such examples are certainly eye-catching let's see how they hold up in actuality. Note inspiration for this section comes from this blog post https://blog.esciencecenter.nl/king-man-woman-king-9a7fd2935a85.\n",
        "\n",
        "Use what you've learned above to test out the following \"equations\":\n",
        "\n",
        "* king - man + woman = queen\n",
        "* bigger - big + cold = colder\n",
        "* Einstein - scientist + Picasso = painter\n",
        "* Paris - France + Italy = Rome\n",
        "* lebron - cavs + lakers = kobe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Wo6SFBIqbhIX"
      },
      "outputs": [],
      "source": [
        "## Code here\n",
        "vec1 = wv['king'] - wv['man'] + wv['woman']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wv.similarity(vec1, 'queen') throws error"
      ],
      "metadata": {
        "id": "3zptFgL_j4w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv.similar_by_vector(vec1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSOTgrUzlR2f",
        "outputId": "3027a643-f9ff-441c-95fd-d4086137391d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king', 0.8449392318725586),\n",
              " ('queen', 0.7300517559051514),\n",
              " ('monarch', 0.645466148853302),\n",
              " ('princess', 0.6156251430511475),\n",
              " ('crown_prince', 0.5818676352500916),\n",
              " ('prince', 0.5777117609977722),\n",
              " ('kings', 0.5613663792610168),\n",
              " ('sultan', 0.5376775860786438),\n",
              " ('Queen_Consort', 0.5344247817993164),\n",
              " ('queens', 0.5289887189865112)]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "tY7JfjefbhIX"
      },
      "outputs": [],
      "source": [
        "vec2 = wv['bigger'] - wv['big'] + wv['cold']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wv.similarity(vec2, 'colder')\n",
        "wv.similar_by_vector(vec2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVBa1edNj7uB",
        "outputId": "92e153cc-8169-4443-e956-751b5eeb69b0"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cold', 0.7878133654594421),\n",
              " ('colder', 0.7053806185722351),\n",
              " ('warmer', 0.6308302283287048),\n",
              " ('colder_temperatures', 0.5584955215454102),\n",
              " ('cooler', 0.5487411022186279),\n",
              " ('chillier', 0.5474385619163513),\n",
              " ('chilly', 0.5405797958374023),\n",
              " ('Cold', 0.5328137874603271),\n",
              " ('frigid', 0.5286216735839844),\n",
              " ('snowier_winter', 0.5251795649528503)]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "yS7LWxCLbhIX"
      },
      "outputs": [],
      "source": [
        "vec3 = wv['Einstein'] - wv['scientist'] + wv['Picasso']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wv.similarity(vec3, 'Rome')\n",
        "wv.similar_by_vector(vec3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxBLQDEWj9YX",
        "outputId": "4d2b5d45-ccc0-46eb-eb20-733b1f8351ac"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Picasso', 0.7582451701164246),\n",
              " ('Pablo_Picasso', 0.596178412437439),\n",
              " ('Matisse', 0.5724692344665527),\n",
              " ('Degas', 0.5545716285705566),\n",
              " ('Einstein', 0.5543243288993835),\n",
              " ('Rembrandt', 0.5465439558029175),\n",
              " ('Dalí', 0.5437686443328857),\n",
              " ('Magritte', 0.5413256883621216),\n",
              " ('Kandinsky', 0.5227084755897522),\n",
              " ('Paul_Cezanne', 0.5170355439186096)]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "HjgCaZ45bhIX"
      },
      "outputs": [],
      "source": [
        "vec4 = wv['lebron'] - wv['cavs'] + wv['lakers']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wv.similarity(vec4, 'kobe')\n",
        "wv.similar_by_vector(vec4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHqLoMfWj_iL",
        "outputId": "3760eafc-f4dd-41e0-e49c-aa33bcc3d47d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('lakers', 0.8796191811561584),\n",
              " ('lebron', 0.7018214464187622),\n",
              " ('laker', 0.612667441368103),\n",
              " ('celtics', 0.6117448210716248),\n",
              " ('shaq', 0.5856783986091614),\n",
              " ('kobe', 0.5744044780731201),\n",
              " ('gasol', 0.5545269846916199),\n",
              " ('bynum', 0.543467104434967),\n",
              " ('nba', 0.5254707336425781),\n",
              " ('mavs', 0.517430305480957)]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT-dIgJBbhIX"
      },
      "source": [
        "## Other Pretrained Models\n",
        "While we've looked at the Google News embedding there are a number of other pretrained Word2Vec embeddings that may be of interest.\n",
        "\n",
        "This Github repository has a nice list of the pretrained models you can get online, https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models.\n",
        "\n",
        "Here are the names from the gensim documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnLyzhWXbhIX"
      },
      "outputs": [],
      "source": [
        "# ['fasttext-wiki-news-subwords-300',\n",
        "#  'conceptnet-numberbatch-17-06-300',\n",
        "#  'word2vec-ruscorpora-300',\n",
        "#  'word2vec-google-news-300',\n",
        "#  'glove-wiki-gigaword-50',\n",
        "#  'glove-wiki-gigaword-100',\n",
        "#  'glove-wiki-gigaword-200',\n",
        "#  'glove-wiki-gigaword-300',\n",
        "#  'glove-twitter-25',\n",
        "#  'glove-twitter-50',\n",
        "#  'glove-twitter-100',\n",
        "#  'glove-twitter-200',\n",
        "#  '__testing_word2vec-matrix-synopsis']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXT4E2gcbhIX"
      },
      "source": [
        "## Building Your Own Model\n",
        "`gensim` also offers the functionality to build your own Word2Vec model if you clean the data up. Let's demonstrate the process with a truly controversial example using Green Eggs and Ham by Dr. Seuss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "emazJ6pgbhIX"
      },
      "outputs": [],
      "source": [
        "## Here is the text of Green Eggs and Ham\n",
        "seuss = '''\n",
        "\"I AM SAM. I AM SAM. SAM I AM.\n",
        "\n",
        "THAT SAM-I-AM! THAT SAM-I-AM! I DO NOT LIKE THAT SAM-I-AM!\n",
        "\n",
        "DO WOULD YOU LIKE GREEN EGGS AND HAM?\n",
        "\n",
        "I DO NOT LIKE THEM,SAM-I-AM.\n",
        "I DO NOT LIKE GREEN EGGS AND HAM.\n",
        "\n",
        "WOULD YOU LIKE THEM HERE OR THERE?\n",
        "\n",
        "I WOULD NOT LIKE THEM HERE OR THERE.\n",
        "I WOULD NOT LIKE THEM ANYWHERE.\n",
        "I DO NOT LIKE GREEN EGGS AND HAM.\n",
        "I DO NOT LIKE THEM, SAM-I-AM.\n",
        "\n",
        "WOULD YOU LIKE THEM IN A HOUSE?\n",
        "WOULD YOU LIKE THEN WITH A MOUSE?\n",
        "\n",
        "I DO NOT LIKE THEM IN A HOUSE.\n",
        "I DO NOT LIKE THEM WITH A MOUSE.\n",
        "I DO NOT LIKE THEM HERE OR THERE.\n",
        "I DO NOT LIKE THEM ANYWHERE.\n",
        "I DO NOT LIKE GREEN EGGS AND HAM.\n",
        "I DO NOT LIKE THEM, SAM-I-AM.\n",
        "\n",
        "WOULD YOU EAT THEM IN A BOX?\n",
        "WOULD YOU EAT THEM WITH A FOX?\n",
        "\n",
        "NOT IN A BOX. NOT WITH A FOX.\n",
        "NOT IN A HOUSE. NOT WITH A MOUSE.\n",
        "I WOULD NOT EAT THEM HERE OR THERE.\n",
        "I WOULD NOT EAT THEM ANYWHERE.\n",
        "I WOULD NOT EAT GREEN EGGS AND HAM.\n",
        "I DO NOT LIKE THEM, SAM-I-AM.\n",
        "\n",
        "WOULD YOU? COULD YOU? IN A CAR?\n",
        "EAT THEM! EAT THEM! HERE THEY ARE.\n",
        "\n",
        "I WOULD NOT, COULD NOT, IN A CAR.\n",
        "\n",
        "YOU MAY LIKE THEM. YOU WILL SEE.\n",
        "YOU MAY LIKE THEM IN A TREE!\n",
        "\n",
        "I WOULD NOT, COULD NOT IN A TREE.\n",
        "NOT IN A CAR! YOU LET ME BE.\n",
        "I DO NOT LIKE THEM IN A BOX.\n",
        "I DO NOT LIKE THEM WITH A FOX.\n",
        "I DO NOT LIKE THEM IN A HOUSE.\n",
        "I DO NOT LIKE THEM WITH A MOUSE.\n",
        "I DO NOT LIKE THEM HERE OR THERE.\n",
        "I DO NOT LIKE THEM ANYWHERE.\n",
        "I DO NOT LIKE GREEN EGGS AND HAM.\n",
        "I DO NOT LIKE THEM, SAM-I-AM.\n",
        "\n",
        "A TRAIN! A TRAIN! A TRAIN! A TRAIN!\n",
        "COULD YOU, WOULD YOU ON A TRAIN?\n",
        "\n",
        "NOT ON TRAIN! NOT IN A TREE!\n",
        "NOT IN A CAR! SAM! LET ME BE!\n",
        "I WOULD NOT, COULD NOT, IN A BOX.\n",
        "I WOULD NOT, COULD NOT, WITH A FOX.\n",
        "I WILL NOT EAT THEM IN A HOUSE.\n",
        "I WILL NOT EAT THEM HERE OR THERE.\n",
        "I WILL NOT EAT THEM ANYWHERE.\n",
        "I DO NOT EAT GREEM EGGS AND HAM.\n",
        "I DO NOT LIKE THEM, SAM-I-AM.\n",
        "\n",
        "SAY! IN THE DARK? HERE IN THE DARK!\n",
        "WOULD YOU, COULD YOU, IN THE DARK?\n",
        "\n",
        "I WOULD NOT, COULD NOT, IN THE DARK.\n",
        "\n",
        "WOULD YOU COULD YOU IN THE RAIN?\n",
        "\n",
        "I WOULD NOT, COULD NOT IN THE RAIN.\n",
        "NOT IN THE DARK. NOT ON A TRAIN.\n",
        "NOT IN A CAR. NOT IN A TREE.\n",
        "I DO NOT LIKE THEM, SAM, YOU SEE.\n",
        "NOT IN A HOUSE. NOT IN A BOX.\n",
        "NOT WITH A MOUSE. NOT WITH A FOX.\n",
        "I WILL NOT EAT THEM HERE OR THERE.\n",
        "I DO NOT LIKE THEM ANYWHERE!\n",
        "\n",
        "YOU DO NOT LIKE GREEN EGGS AND HAM?\n",
        "\n",
        "I DO NOT LIKE THEM, SAM-I-AM.\n",
        "\n",
        "COULD YOU, WOULD YOU, WITH A GOAT?\n",
        "\n",
        "I WOULD NOT, COULD NOT WITH A GOAT!\n",
        "\n",
        "WOULD YOU, COULD YOU, ON A BOAT?\n",
        "\n",
        "I COULD NOT, WOULD NOT, ON A BOAT.\n",
        "I WILL NOT, WILL NOT, WITH A GOAT.\n",
        "I WILL NOT EAT THEM IN THE RAIN.\n",
        "NOT IN THE DARK! NOT IN A TREE!\n",
        "NOT IN A CAR! YOU LET ME BE!\n",
        "I DO NOT LIKE THEM IN A BOX.\n",
        "I DO NOT LIKE THEM WITH A FOX.\n",
        "I WILL NOT EAT THEM IN A HOUSE.\n",
        "I DO NOT LIKE THEM WITH A MOUSE.\n",
        "I DO NOT LIKE THEM HERE OR THERE.\n",
        "I DO NOT LIKE THEM ANYWHERE!\n",
        "I DO NOT LIKE GREEN EGGS AND HAM!\n",
        "I DO NOT LIKE THEM, SAM-I-AM.\n",
        "\n",
        "YOU DO NOT LIKE THEM. SO YOU SAY.\n",
        "TRY THEM! TRY THEM! AND YOU MAY.\n",
        "TRY THEM AND YOU MAY, I SAY.\n",
        "\n",
        "sAM! IF YOU LET ME BE,\n",
        "I WILL TRY THEM. YOU WILL SEE.\n",
        "\n",
        "(... and he tries them ...)\n",
        "\n",
        "SAY! I LIKE GREEN EGGS AND HAM!\n",
        "I DO! I LIKE THEM, SAM-I-AM!\n",
        "AND I WOULD EAT THEM IN A BOAT.\n",
        "AND I WOULD EAT THEM WITH A GOAT...\n",
        "AND I WILL EAT THEM, IN THE RAIN.\n",
        "AND IN THE DARK. AND ON A TRAIN.\n",
        "AND IN A CAR. AND IN A TREE.\n",
        "THEY ARE SO GOOD, SO GOOD, YOU SEE!\n",
        "SO I WILL EAT THEM IN A BOX.\n",
        "AND I WILL EAT THEM WITH A FOX.\n",
        "AND I WILL EAT THEM IN A HOUSE.\n",
        "AND I WILL EAT THEM WITH A MOUSE.\n",
        "AND I WILL EAT THEM HERE AND THERE.\n",
        "SAY! I WILL EAT THEM ANYWHERE!\n",
        "I DO SO LIKE GREEN EGGS AND HAM!\n",
        "THANK YOU! THANK YOU, SAM I AM.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "rPx9EWuKbhIY"
      },
      "outputs": [],
      "source": [
        "# Here I clean it up a bit\n",
        "seuss = seuss.replace(\"\\n\",\"\").replace(\",\",\" \").replace('\"',\"\").replace(\"(\",\"\").replace(\")\",\"\")\n",
        "seuss = seuss.replace(\".\",\". \").replace(\"!\",\"! \").replace(\"?\",\"? \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-le7OcAbhIY",
        "outputId": "0f61329b-b6ac-4776-8690-a5b718a13d62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I AM SAM.  I AM SAM.  SAM I AM. THAT SAM-I-AM!  THAT SAM-I-AM!  I DO NOT LIKE THAT SAM-I-AM! DO WOULD YOU LIKE GREEN EGGS AND HAM? I DO NOT LIKE THEM SAM-I-AM. I DO NOT LIKE GREEN EGGS AND HAM. WOULD YOU LIKE THEM HERE OR THERE? I WOULD NOT LIKE THEM HERE OR THERE. I WOULD NOT LIKE THEM ANYWHERE. I DO NOT LIKE GREEN EGGS AND HAM. I DO NOT LIKE THEM  SAM-I-AM. WOULD YOU LIKE THEM IN A HOUSE? WOULD YOU LIKE THEN WITH A MOUSE? I DO NOT LIKE THEM IN A HOUSE. I DO NOT LIKE THEM WITH A MOUSE. I DO NOT LIKE THEM HERE OR THERE. I DO NOT LIKE THEM ANYWHERE. I DO NOT LIKE GREEN EGGS AND HAM. I DO NOT LIKE THEM  SAM-I-AM. WOULD YOU EAT THEM IN A BOX? WOULD YOU EAT THEM WITH A FOX? NOT IN A BOX.  NOT WITH A FOX. NOT IN A HOUSE.  NOT WITH A MOUSE. I WOULD NOT EAT THEM HERE OR THERE. I WOULD NOT EAT THEM ANYWHERE. I WOULD NOT EAT GREEN EGGS AND HAM. I DO NOT LIKE THEM  SAM-I-AM. WOULD YOU?  COULD YOU?  IN A CAR? EAT THEM!  EAT THEM!  HERE THEY ARE. I WOULD NOT  COULD NOT  IN A CAR. YOU MAY LIKE THEM.  YOU WILL SEE. YOU MAY LIKE THEM IN A TREE! I WOULD NOT  COULD NOT IN A TREE. NOT IN A CAR!  YOU LET ME BE. I DO NOT LIKE THEM IN A BOX. I DO NOT LIKE THEM WITH A FOX. I DO NOT LIKE THEM IN A HOUSE. I DO NOT LIKE THEM WITH A MOUSE. I DO NOT LIKE THEM HERE OR THERE. I DO NOT LIKE THEM ANYWHERE. I DO NOT LIKE GREEN EGGS AND HAM. I DO NOT LIKE THEM  SAM-I-AM. A TRAIN!  A TRAIN!  A TRAIN!  A TRAIN! COULD YOU  WOULD YOU ON A TRAIN? NOT ON TRAIN!  NOT IN A TREE! NOT IN A CAR!  SAM!  LET ME BE! I WOULD NOT  COULD NOT  IN A BOX. I WOULD NOT  COULD NOT  WITH A FOX. I WILL NOT EAT THEM IN A HOUSE. I WILL NOT EAT THEM HERE OR THERE. I WILL NOT EAT THEM ANYWHERE. I DO NOT EAT GREEM EGGS AND HAM. I DO NOT LIKE THEM  SAM-I-AM. SAY!  IN THE DARK?  HERE IN THE DARK! WOULD YOU  COULD YOU  IN THE DARK? I WOULD NOT  COULD NOT  IN THE DARK. WOULD YOU COULD YOU IN THE RAIN? I WOULD NOT  COULD NOT IN THE RAIN. NOT IN THE DARK.  NOT ON A TRAIN. NOT IN A CAR.  NOT IN A TREE. I DO NOT LIKE THEM  SAM  YOU SEE. NOT IN A HOUSE.  NOT IN A BOX. NOT WITH A MOUSE.  NOT WITH A FOX. I WILL NOT EAT THEM HERE OR THERE. I DO NOT LIKE THEM ANYWHERE! YOU DO NOT LIKE GREEN EGGS AND HAM? I DO NOT LIKE THEM  SAM-I-AM. COULD YOU  WOULD YOU  WITH A GOAT? I WOULD NOT  COULD NOT WITH A GOAT! WOULD YOU  COULD YOU  ON A BOAT? I COULD NOT  WOULD NOT  ON A BOAT. I WILL NOT  WILL NOT  WITH A GOAT. I WILL NOT EAT THEM IN THE RAIN. NOT IN THE DARK!  NOT IN A TREE! NOT IN A CAR!  YOU LET ME BE! I DO NOT LIKE THEM IN A BOX. I DO NOT LIKE THEM WITH A FOX. I WILL NOT EAT THEM IN A HOUSE. I DO NOT LIKE THEM WITH A MOUSE. I DO NOT LIKE THEM HERE OR THERE. I DO NOT LIKE THEM ANYWHERE! I DO NOT LIKE GREEN EGGS AND HAM! I DO NOT LIKE THEM  SAM-I-AM. YOU DO NOT LIKE THEM.  SO YOU SAY. TRY THEM!  TRY THEM!  AND YOU MAY. TRY THEM AND YOU MAY  I SAY. sAM!  IF YOU LET ME BE I WILL TRY THEM.  YOU WILL SEE. . . .  and he tries them . . . SAY!  I LIKE GREEN EGGS AND HAM! I DO!  I LIKE THEM  SAM-I-AM! AND I WOULD EAT THEM IN A BOAT. AND I WOULD EAT THEM WITH A GOAT. . . AND I WILL EAT THEM  IN THE RAIN. AND IN THE DARK.  AND ON A TRAIN. AND IN A CAR.  AND IN A TREE. THEY ARE SO GOOD  SO GOOD  YOU SEE! SO I WILL EAT THEM IN A BOX. AND I WILL EAT THEM WITH A FOX. AND I WILL EAT THEM IN A HOUSE. AND I WILL EAT THEM WITH A MOUSE. AND I WILL EAT THEM HERE AND THERE. SAY!  I WILL EAT THEM ANYWHERE! I DO SO LIKE GREEN EGGS AND HAM! THANK YOU!  THANK YOU  SAM I AM. \n"
          ]
        }
      ],
      "source": [
        "# The clean version\n",
        "print(seuss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "g89g9UlIbhIY"
      },
      "outputs": [],
      "source": [
        "# gensim only requires sequences of words in a sentence\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqLTEHrzicLn",
        "outputId": "ba02165c-abe9-4342-d937-7c4c7bdb7b68"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "INrjNd3TbhIY"
      },
      "outputs": [],
      "source": [
        "# let's give them just that\n",
        "sentences = [word_tokenize(sent) for sent in sent_tokenize(seuss)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "XaH8uupabhIY"
      },
      "outputs": [],
      "source": [
        "# You train a Word2Vec model in gensim with\n",
        "# Word2Vec\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea3SWY4QbhIY"
      },
      "source": [
        "### Parameters\n",
        "\n",
        "* `min_count` = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
        "* `window` = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n",
        "* `size` = int - Dimensionality of the feature vectors. - (50, 300)\n",
        "* `sample` = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n",
        "* `alpha` = float - The initial learning rate - (0.01, 0.05)\n",
        "* `min_alpha` = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
        "* `negative` = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
        "* `workers` = int - Use these many worker threads to train the model (=faster training with multicore machines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "BK68DAqFbhIY"
      },
      "outputs": [],
      "source": [
        "# This line makes a model\n",
        "# You first put in your sentences\n",
        "# then you can OPTIONALLY specify a number of parameters\n",
        "# vector_size is the size of the hidden layer\n",
        "# window is the size of the skip-gram window\n",
        "# min_count sets a minimum number of times that a word has to appear\n",
        "model = Word2Vec(sentences = sentences,\n",
        "                     vector_size=10,\n",
        "                     window=1,\n",
        "                     min_count=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdYAssslbhIY",
        "outputId": "e9fdc874-5629-4e1d-bb02-2f5b6d8b51d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.03042409,  0.05153794,  0.0650674 , -0.03252598,  0.07675114,\n",
              "        0.02693785,  0.03590595, -0.02004471, -0.03641359, -0.02361511],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "model.wv['SAM']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Rq8UNozbhIZ",
        "outputId": "cdab40fa-847d-4a05-b86f-e234bc9e6ae2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.04845909, -0.03268769,  0.09400992,  0.00452371,  0.07864018,\n",
              "       -0.06110672,  0.05772606,  0.10262594, -0.08987087, -0.0504397 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "model.wv['EGGS']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bwL47G5bhIc",
        "outputId": "438a45f6-fdda-42e9-a69d-e619f866eee2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('TRY', 0.7984501123428345),\n",
              " ('ON', 0.6117178797721863),\n",
              " ('IN', 0.5327495336532593),\n",
              " ('YOU', 0.47896048426628113),\n",
              " ('LIKE', 0.4073309600353241),\n",
              " ('A', 0.40630805492401123),\n",
              " ('TRAIN', 0.40030962228775024),\n",
              " ('AND', 0.37113553285598755),\n",
              " ('GREEN', 0.3708000183105469),\n",
              " ('EAT', 0.3582375943660736)]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "model.wv.most_similar(['HAM'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuh4-EegbhId"
      },
      "source": [
        "Since we spent so much time and energy training this model we may want to save it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ZvhlyjFSbhId"
      },
      "outputs": [],
      "source": [
        "model.save(\"green_eggs_and_ham.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E8AFXO6bhId"
      },
      "source": [
        "We can load it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "JjZmpqiabhId"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec.load(\"green_eggs_and_ham.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQSFgZKtbhId"
      },
      "source": [
        "Now when training an actual model you'll likely want to know all the bells and whistles for tuning the model. You can find all that and more here: https://radimrehurek.com/gensim/models/word2vec.html?highlight=word2vec#module-gensim.models.word2vec\n",
        "\n",
        "## An Improvement on Word2Vec\n",
        "A nice improvement on Word2Vec was made by Facebook researchers (including Mikolov) called, fasttext, https://arxiv.org/abs/1607.04606.\n",
        "\n",
        "The key idea to the improvement was to replace words and phrases as the base unit input layer with partial character -grams of words. For example instead of \"apples\" they used things like \"app\", \"ppl\", \"ple\", and \"les\".\n",
        "\n",
        "This is also implementable with gensim. Here is the documentation, https://radimrehurek.com/gensim_3.8.3/models/fasttext.html.\n",
        "\n",
        "Below I'll demonstrate Facebook's pretrained model trained on Wikipedia. Note don't try and run this code right now because it takes a long time to download the model. Just watch the code, and if you're interested in the model you can always play with it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "mX1HU7f9bhId",
        "outputId": "d0100951-b902-4f13-b5e5-080e7100649c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-e84c66ec071b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fasttext-wiki-news-subwords-300'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/gensim-data/fasttext-wiki-news-subwords-300/__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fasttext-wiki-news-subwords-300'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fasttext-wiki-news-subwords-300.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             )\n\u001b[1;32m   2068\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2069\u001b[0;31m             \u001b[0m_word2vec_read_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2070\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m         logger.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_read_text\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1972\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1974\u001b[0;31m         \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1975\u001b[0m         \u001b[0m_add_word_to_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_line_to_vector\u001b[0;34m(line, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1978\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1980\u001b[0;31m     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1981\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1978\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1980\u001b[0;31m     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1981\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "ft = api.load('fasttext-wiki-news-subwords-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvTvOYZwbhId"
      },
      "outputs": [],
      "source": [
        "## Let's look at the top 20 words\n",
        "for i in range(20):\n",
        "    print(ft.index_to_key[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-pl4Mg_bhId"
      },
      "outputs": [],
      "source": [
        "## And just to note we can use this model in exactly\n",
        "## the same way as Word2Vec\n",
        "ft.similar_by_word(\"apple\",topn=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGFBSaUtbhId"
      },
      "outputs": [],
      "source": [
        "ft.similar_by_word(\"ppl\",topn=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUqgDQnjbhId"
      },
      "outputs": [],
      "source": [
        "test = ft['king'] - ft['man'] + ft['woman']\n",
        "ft.similar_by_vector(test,topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRCIJl5JbhIe"
      },
      "source": [
        "That's it for this notebook. I hope you enjoyed playing around with `gensim`'s word embedding models!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}